<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CHIDEOD | Thomas Van Hoey | Sinologica</title>
    <link>/tags/chideod/</link>
      <atom:link href="/tags/chideod/index.xml" rel="self" type="application/rss+xml" />
    <description>CHIDEOD</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Thomas Van Hoey</copyright><lastBuildDate>Mon, 09 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>CHIDEOD</title>
      <link>/tags/chideod/</link>
    </image>
    
    <item>
      <title>Data packages for current and future me</title>
      <link>/post/data-packages/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/data-packages/</guid>
      <description>

&lt;h1 id=&#34;tl-dr&#34;&gt;tl; dr&lt;/h1&gt;

&lt;p&gt;I show why it is worthwile to put my Chinese-related datasets in packages and how I went about it.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;I don&amp;rsquo;t know if I&amp;rsquo;m very late to the party, but in this final sprint towards a finished dissertation, I keep finding myself juggling multiple datasets when using them in R.
This usually is paired with a &lt;code&gt;readr::read_csv()&lt;/code&gt; or related functions, but the drawback is that I have to go find where I actually put that dataset on my computer.
This causes me some friction (even though the location didn&amp;rsquo;t change), but what&amp;rsquo;s worse is that that&amp;rsquo;s actually not reproducible.
In other words, the code I wrote that way is &lt;strong&gt;not portable&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&#34;first-trial-chideod&#34;&gt;First trial: CHIDEOD&lt;/h1&gt;

&lt;p&gt;At some point during this year, I suddenly remembered some blog posts I had read like a year ago, on how to turn your dataset into an R package*.
My first experiment was the database I&amp;rsquo;m making myself, the &lt;strong&gt;Chinese Ideophone Database&lt;/strong&gt; (CHIDEOD; available &lt;a href=&#34;https://osf.io/kpwgf/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).
Somewhere in March/April this year I managed to turn that into a data package, and have done so with subesquent versions of CHIDEOD.
It provided me with the functionality of just running a &lt;code&gt;library(CHIDEOD)&lt;/code&gt; and BAM!, there was the &lt;code&gt;chideod&lt;/code&gt; object to my disposal.&lt;/p&gt;

&lt;p&gt;*More precisely the blog posts I read were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ed Hagen&amp;rsquo;s &lt;a href=&#34;https://grasshoppermouse.github.io/2017/10/18/put-your-data-in-an-r-package/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Put your data into an R package&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dave Kleinschmidt&amp;rsquo;s &lt;a href=&#34;https://www.davekleinschmidt.com/r-packages/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Taking your data to go with R packages&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Erick Howard&amp;rsquo;s &lt;a href=&#34;https://www.erikhoward.net/how-to-create-an-r-data-package/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;How to create an R data package&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hilary Parker&amp;rsquo;s &lt;a href=&#34;https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;Writing an R package from scratch&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;other-datasets&#34;&gt;Other datasets&lt;/h1&gt;

&lt;p&gt;Then somewhere in the beginning of this semester something clicked: I would take the other datasets I&amp;rsquo;m often working with, and find a way to turn them into data packages as well.
In concreto, those datasets are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ocbaxtersagart.lsait.lsa.umich.edu&#34; target=&#34;_blank&#34;&gt;Baxter and Sagart&amp;rsquo;s Middle Chinese and Old Chinese reconstructions&lt;/a&gt;. This was all the more important, because this website has been unavailble at times &lt;em&gt;(maybe because there is no https certificate?, I don&amp;rsquo;t know)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.chineselexicaldatabase.com&#34; target=&#34;_blank&#34;&gt;Sun et al.&amp;rsquo;s Chinese Lexical Database&lt;/a&gt;, a very good database for psycholinguistic information on simplified Chinese characters.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;Hànyǔ dà cídiǎn&lt;/em&gt; 漢語大詞典, a giant monolingual dictionary of Chinese.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;http://asbc.iis.sinica.edu.tw&#34; target=&#34;_blank&#34;&gt;Academia Sinica Balanced Corpus of Chinese ASBC 4.0&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, I need to mention that for my own purposes, I have made some alterations to all of these datasets.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For Baxter &amp;amp; Sagart&amp;rsquo;s dataset, I&amp;rsquo;ve added things like an &lt;a href=&#34;https://en.wikipedia.org/wiki/International_Phonetic_Alphabet&#34; target=&#34;_blank&#34;&gt;IPA&lt;/a&gt; version of the Middle Chinese (thanks to &lt;a href=&#34;https://pypi.org/project/sinopy/&#34; target=&#34;_blank&#34;&gt;sinopy&lt;/a&gt;), a simplified column next to the traditional character one etc.&lt;/li&gt;
&lt;li&gt;For the Chinese Lexical Database, which is based on simplified characters, I found it useful to still add a traditional column and pull the characters of each word into traditional characters as well. Most of the psycholinguistic measures (entropy, reaction time etc.) are only valid for the simplified ones, but there is still a wealth of information that is useful for the traditional ones.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve &lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html&#34; target=&#34;_blank&#34;&gt;tidied up&lt;/a&gt; the &lt;em&gt;Hànyǔ dà cídiǎn&lt;/em&gt; 漢語大詞典, by splitting all the meanings that were numbered per lemma.&lt;/li&gt;
&lt;li&gt;I was lucky to access an institutional version of the ASBC 4.0 but it was unfortunately coded in the programming-language-neutral XML, which is not fun to use (&lt;em&gt;&amp;ldquo;XML IS HELL&amp;rdquo;&lt;/em&gt;). So I spent like a day turning it into nicely structured .Rds files. Not language-neutral (R-friendly) but easier to use for me. I also changed the spacing of the &amp;ldquo;Chines big space&amp;rdquo; (the one that is one square long and tall) to normal spacing; on top of the ugly tagging method of using brackets, e.g. 我(PRON), to less visually cluttered underscores, i.e. 我_PRON.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-12-09-data-packages_files/yodar.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;storing-them-somewhere&#34;&gt;Storing them somewhere&lt;/h1&gt;

&lt;p&gt;In an ideal scenario I would do this step first, but I actually did the package stuff (see next section) first.
In this step I uploaded my datasets to a (private) repository.
Since my experience with the &lt;a href=&#34;https://osf.io&#34; target=&#34;_blank&#34;&gt;Open Science Framework OSF&lt;/a&gt; had been positive, I decided to dump my datasets there.
This has the advantages that I can 1) version them, 2) access them from different devices (in case of computer crashes or changes etc.).
So now I need to find out how to shareable these new datasets are.
I&amp;rsquo;m pretty sure I can&amp;rsquo;t just make public the dictionary (even though I have transformed it), or the corpus &amp;ndash; although I think that these projects should be freely available.
The dataset by Baxter &amp;amp; Sagart and the Chinese Lexical Database should be a bit more available, but I will contact the authors shortly about these issues and when they are available, set the packages repositories on gitub to public, so you can all enjoy them.&lt;/p&gt;

&lt;h1 id=&#34;how-to-make-a-datapackage&#34;&gt;How to make a datapackage&lt;/h1&gt;

&lt;p&gt;The main tool I used to create these datapackages is the &lt;code&gt;DataPackageR&lt;/code&gt; package.
&lt;a href=&#34;https://ropensci.org/blog/2018/09/18/datapackager/&#34; target=&#34;_blank&#34;&gt;This post on the ROpenSci blog&lt;/a&gt; explains pretty well the why and how, and I actually just used the &amp;ldquo;copy + adapt no &lt;a href=&#34;https://naruto.fandom.com/wiki/Jutsu&#34; target=&#34;_blank&#34;&gt;jutsu&lt;/a&gt;&amp;rdquo; to make my packages.
It does warrant a basic know-how of R(studio) and github, so if you need to brush up on those skills, I can recommend Jenny Bryan&amp;rsquo;s &lt;a href=&#34;https://happygitwithr.com&#34; target=&#34;_blank&#34;&gt;Happy Git and GitHub for the useR&lt;/a&gt;, a.k.a. &lt;em&gt;Happy git with R&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So that&amp;rsquo;s what I did with most of the packages.
To see the one that is already public &amp;ndash; my CHIDEOD &amp;ndash; you can &lt;a href=&#34;https://github.com/simazhi/chinese_ideophone_database&#34; target=&#34;_blank&#34;&gt;go to this page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And to download it, you should be able to do these functions&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(devtools)
install_github(&amp;quot;simazhi/chinese_ideophone_database/CHIDEOD&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;If you find any problems with this functionality, please contact me!&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;challenges&#34;&gt;Challenges&lt;/h1&gt;

&lt;p&gt;I don&amp;rsquo;t know how to make the data package for the large-ish dataset of the &lt;em&gt;Hànyǔ dà cídiǎn&lt;/em&gt; 漢語大詞典 (some 200 MB), since it is bigger than Github&amp;rsquo;s size allowance.
I just &lt;a href=&#34;https://thecoatlessprofessor.com/programming/r/r-data-packages-in-external-data-repositories-using-the-additional_repositories-field/&#34; target=&#34;_blank&#34;&gt;found this blog on attaching larger datasets&lt;/a&gt;, so I&amp;rsquo;m gonna check that out at some point.
Also the ASBC corpus, I have just stored in the repository, it&amp;rsquo;s probably safer there.&lt;/p&gt;

&lt;p&gt;Another challenge is naming the packages.
I&amp;rsquo;ve settled for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CHIDEOD&lt;/li&gt;
&lt;li&gt;CLD&lt;/li&gt;
&lt;li&gt;baxteRsagaRt&lt;/li&gt;
&lt;li&gt;(&lt;em&gt;future:&lt;/em&gt; HYDCD)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;After updating to a new version of R (I read that R 4.0 is around the corner), and / or updating to a newer version of Rstudio, the dance of reinstalling previously installed packages will befall us once again.
But knowing that this includes (re)installing these datasets I made and that I often use, gives me some solace, because I (will?) have looked ahead to that moment and can just do a simple &lt;code&gt;install_github&lt;/code&gt; and a &lt;code&gt;library(PACKAGE)&lt;/code&gt; and &lt;em&gt;badabing badaboom&lt;/em&gt;: I can run my scripts, in the now and in the future.&lt;/p&gt;

&lt;p&gt;If the aforementioned parties are okay with it, I will make my github data packages public and share them in this post, so keep an eye out for that yo!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Guanguan goes the Chinese Word Segmentation (II)</title>
      <link>/post/guanguan-goes-the-chinese-word-segmentation-2/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/guanguan-goes-the-chinese-word-segmentation-2/</guid>
      <description>

&lt;h1 id=&#34;tl-dr&#34;&gt;tl; dr&lt;/h1&gt;

&lt;p&gt;This double blog is first about the opening line of the &lt;em&gt;Book of Odes&lt;/em&gt;, and later about how to deal with Chinese word segmentation, and my current implementation of it. So if you&amp;rsquo;re only &lt;a href=&#34;../guanguan-goes-the-chinese-word-segmentation-2&#34;&gt;interested in the computational part, look at the next one&lt;/a&gt;. If, on the other hand, you want to know more about my views on the translation of &lt;em&gt;guān guān&lt;/em&gt; etc., &lt;a href=&#34;../guanguan-goes-the-chinese-word-segmentation&#34;&gt;look at the first part&lt;/a&gt;.
In this part I use different approaches from a mostly R-centred focus to look at word segmentation in Chinese.&lt;/p&gt;

&lt;h1 id=&#34;intro-quick-recap&#34;&gt;Intro - quick recap&lt;/h1&gt;

&lt;p&gt;The issues that prompted me to write the previous post are twofold.
On the one hand, I came across a translation of the first line of the &lt;em&gt;Book of Odes&lt;/em&gt; (&lt;em&gt;Shījīng&lt;/em&gt; 詩經) and subsequent critiques of that translation.
I decided to give my take on the extensive coverage of the first stanza.
Here is the first line as it is transmitted to us, as well with a translation I made.
(By the way, if you can provide a better translation, please bring it on.)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;關關雎鳩，&lt;br /&gt;
在河之洲。&lt;br /&gt;
窈窕淑女，&lt;br /&gt;
君子好逑。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Krōn, krōn&lt;/em&gt; the físh-hawks cáll,&lt;br /&gt;
ón the íslet ín the ríver.&lt;br /&gt;
délicáte, demúre, young lády,&lt;br /&gt;
fór the lórd a góód mate shé.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On the other hand, I have been reading up on how to deal with Chinese from a corpus linguistics point-of-view.
And that means also looking at how Natural Language Processing &amp;ndash; the next-door-neighbour of corpus linguistics &amp;ndash; deals with these issues.
To see why they are next-door-neighbours and not the same, you can read Gries&amp;rsquo;s (2011) &amp;ldquo;Methodological and interdisciplinary stance in Corpus Linguistics&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Anyway, there are &lt;em&gt;a lot&lt;/em&gt; of things you can do with corpora, also in Chinese, but in general there are a few steps you need to do before you can even begin analyzing linguistic data and throwing different models at the data.&lt;/p&gt;

&lt;h1 id=&#34;steps-involved-in-text-analysis&#34;&gt;Steps involved in text analysis&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;10.1080/19312458.2017.1387238&#34; target=&#34;_blank&#34;&gt;Welbers et al. (2017)&lt;/a&gt; discuss the steps generally involved in text analysis, with a particular focus on &lt;a href=&#34;https://www.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt;.
These can be subsumed in three groups of tasks, and are exemplified with some R packages that may do the trick for that particular task.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/2019/Welbers2017.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But&lt;/strong&gt;, what they forgot is that not every language comes with nicely defined space between the units (not necessarily words, because that concept is also a bit fuzzy).
Chinese and Japanese are prime examples of this phenomenon.
A really quick google search led me to this &lt;a href=&#34;https://www.quora.com/What-are-the-main-differences-between-NLP-for-Chinese-vs-NLP-for-English&#34; target=&#34;_blank&#34;&gt;Quora post where they asked what the differences are between Chinese and English NLP (Natural Language Processing)&lt;/a&gt;, and the answers, provided a certain Chier Hu are pretty good (&lt;a href=&#34;https://qr.ae/TWKny4&#34; target=&#34;_blank&#34;&gt;check it out yo&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;What I&amp;rsquo;m trying to get at here is that you need to break up long strings of Chinese first before you can even about putting things in the language modelling mixer.
So &lt;strong&gt;below I am going to discuss a bit how I went about SEGMENTATION in the past, what some alternatives are, and what I&amp;rsquo;m doing now.&lt;/strong&gt;
&lt;strong&gt;If you have any suggestions to improve this workflow, please contact me!&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;alternatives&#34;&gt;Alternatives&lt;/h1&gt;

&lt;h2 id=&#34;the-monosyllabic-approach&#34;&gt;The monosyllabic approach&lt;/h2&gt;

&lt;p&gt;What I&amp;rsquo;m looking for is actually not just a segmentation tool for Modern Chinese (which is difficult enough), but I want one that also works for Classical Chinese / Old Chinese.
The easiest option is to go with the idea that Classical Chinese is &lt;em&gt;mostly&lt;/em&gt; monosyllabic (one syllable = one character = one word).
If that is true, you can just go with the Julia Silge&amp;rsquo;s brilliant &lt;code&gt;tidytext&lt;/code&gt; package (&lt;a href=&#34;https://www.tidytextmining.com&#34; target=&#34;_blank&#34;&gt;see free e-book here&lt;/a&gt;), and set your &lt;code&gt;token = &amp;quot;characters&amp;quot;&lt;/code&gt;.
That this is a possible venue is shown here by a certain jjon987, &lt;a href=&#34;http://jjohn987.rbind.io/post/a-quasi-tidytext-analysis-of-3-chinese-classics/&#34; target=&#34;_blank&#34;&gt;who does an exploratory analysis of some classics in Chinese&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I would give that an A for exploratory analysis, but a B for segmentation.
That is because what I&amp;rsquo;m looking for, is something that is able to also deal with polysyllabic words like ideophones, e.g. &lt;em&gt;guānguān&lt;/em&gt; 關關 and &lt;em&gt;yáotiáo&lt;/em&gt; 窈窕, or polysyllabic monomorphemes like &lt;em&gt;jūnzi&lt;/em&gt; 君子, all present in this first stanza of the &lt;em&gt;Shījīng&lt;/em&gt;.
So we need something a bit more sophisiticated.&lt;/p&gt;

&lt;h2 id=&#34;stringi-based-approaches&#34;&gt;&lt;code&gt;stringi&lt;/code&gt; based approaches&lt;/h2&gt;

&lt;p&gt;Both the &lt;a href=&#34;https://quanteda.io/index.html&#34; target=&#34;_blank&#34;&gt;quanteda&lt;/a&gt; package and the &lt;a href=&#34;https://cran.r-project.org/web/packages/corpus/vignettes/corpus.html&#34; target=&#34;_blank&#34;&gt;corpus&lt;/a&gt; package do make use of the &lt;code&gt;stringi&lt;/code&gt; package to deal with the segmentation of Japanese and/or Chinese.
For an implementation of &lt;code&gt;quanteda&lt;/code&gt; on Japanese, I really recommend looking at &lt;a href=&#34;https://koheiw.net/?p=339&#34; target=&#34;_blank&#34;&gt;Kohei Watanabe&amp;rsquo;s post&lt;/a&gt;; for the instructions regarding &lt;code&gt;corpus&lt;/code&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/corpus/vignettes/chinese.html&#34; target=&#34;_blank&#34;&gt;look here&lt;/a&gt;.
Because I&amp;rsquo;m most familiar with the &lt;code&gt;quanteda&lt;/code&gt; package and its functions, and because the underlying mechenism is the same, I&amp;rsquo;m only going to discuss this package below in &amp;ldquo;the test&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;(I think &lt;code&gt;tidytext&lt;/code&gt;&amp;rsquo;s &lt;code&gt;token = &amp;quot;words&amp;quot;&lt;/code&gt; also uses &lt;code&gt;stringi&lt;/code&gt; but I&amp;rsquo;m not sure.)&lt;/p&gt;

&lt;h2 id=&#34;oldies-but-goldies&#34;&gt;oldies but goldies&lt;/h2&gt;

&lt;p&gt;The package most people are familiar with is probably &lt;code&gt;jieba&lt;/code&gt; (&lt;a href=&#34;https://qinwenfeng.com/jiebaR/&#34; target=&#34;_blank&#34;&gt;R version&lt;/a&gt; and &lt;a href=&#34;https://github.com/fxsjy/jieba&#34; target=&#34;_blank&#34;&gt;python version&lt;/a&gt;).
This tends to work pretty well, but to be honest, it has always worked better for me in python, especially when I want to add custom dictionaries.
About those dictionaries, I don&amp;rsquo;t know why, but often they are not enforced, and that is actually a dealbreaker for me.
Word on the street also has it that it works much better for Simplfied Chinese than for Traditional Chinese, but I haven&amp;rsquo;t subjected this to tests myself.&lt;/p&gt;

&lt;h2 id=&#34;recent-approaches&#34;&gt;recent approaches&lt;/h2&gt;

&lt;p&gt;These last few years &lt;a href=&#34;http://nlpprogress.com/chinese/chinese_word_segmentation.html&#34; target=&#34;_blank&#34;&gt;numerous models have been introduced&lt;/a&gt;, each outperforming the previous one by one percent or so.
However, I wonder how much (corpus) linguists would agree with the standards from NLP &amp;ndash; there still seems to be a slightly more critical approach to the foundations of the issues.&lt;/p&gt;

&lt;p&gt;That being said, most of these newer models include datasets that have benefited from neural networks etc. The &lt;a href=&#34;https://bnosac.github.io/udpipe/docs/doc1.html&#34; target=&#34;_blank&#34;&gt;udpipe&lt;/a&gt; package brands itself as belonging to that category, and is thus worth exploring, especially since they have &lt;code&gt;classical_chinese-kyoto&lt;/code&gt; dataset that can help you segment and tokenize your data.
I&amp;rsquo;m curious if they can live up to the promises they make.&lt;/p&gt;

&lt;h2 id=&#34;python-integrated-approaches&#34;&gt;python-integrated approaches&lt;/h2&gt;

&lt;p&gt;Last but not least is the group of R-and-python interfacing packages, all made possible (to me at least) through the &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34; target=&#34;_blank&#34;&gt;reticulate package&lt;/a&gt;.
With this I can basically run python from inside one of my R (markdown) scripts, and thus get the best of both worlds.
It&amp;rsquo;s a bit of a hassle to set up at first, but if you manage to do it, the rewars are pretty sweet.&lt;/p&gt;

&lt;p&gt;As for &lt;del&gt;packages&lt;/del&gt; libraries (python lingo), the &lt;a href=&#34;https://github.com/fxsjy/jieba&#34; target=&#34;_blank&#34;&gt;jieba library&lt;/a&gt; works pretty well.
But last week the CKIP team at Academia Sinica came out with this new tagging system, creatively called &lt;a href=&#34;https://github.com/ckiplab/ckiptagger&#34; target=&#34;_blank&#34;&gt;ckiptagger&lt;/a&gt;.
At first sight, this one does seem to be able to enforce a dictionary, so maybe this is the one I want to be using.
Let&amp;rsquo;s take these for a spin.&lt;/p&gt;

&lt;h1 id=&#34;setting-up-the-workspace-and-test-materials&#34;&gt;Setting up the workspace and test materials&lt;/h1&gt;

&lt;p&gt;So in this part I want to showcase a bit how different packages treat the question of segmentation.&lt;/p&gt;

&lt;h2 id=&#34;loading-in-the-required-libraries&#34;&gt;Loading in the required libraries&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse) #general catch-all of the tidyverse
library(quanteda)
library(tidytext)
library(jiebaR)
library(udpipe)

# python setup
library(reticulate)
use_python(&amp;quot;/usr/local/bin/python3&amp;quot;, required = T)
reticulate::py_config()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## python:         /usr/local/bin/python3
## libpython:      /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/config-3.7m-darwin/libpython3.7.dylib
## pythonhome:     /Library/Frameworks/Python.framework/Versions/3.7:/Library/Frameworks/Python.framework/Versions/3.7
## version:        3.7.0 (v3.7.0:1bf9cc5093, Jun 26 2018, 23:26:24)  [Clang 6.0 (clang-600.0.57)]
## numpy:          /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy
## numpy_version:  1.17.1
## 
## NOTE: Python version was forced by use_python function
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;test-material&#34;&gt;Test material&lt;/h2&gt;

&lt;p&gt;As test material I just care about this stanza from the &lt;em&gt;Shījīng&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;test &amp;lt;- c(&amp;quot;關關雎鳩、在河之洲。&amp;quot;,
          &amp;quot;窈窕淑女、君子好逑。&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Expected output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;關關 雎鳩 、 在 河 之 洲 。
窈窕 淑女 、 君子 好 逑 。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;tidytext&#34;&gt;&lt;code&gt;tidytext&lt;/code&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;test %&amp;gt;%
  tibble(.name_repair = ~ &amp;quot;lines&amp;quot;) %&amp;gt;%
  unnest_tokens(word, lines, token = &amp;quot;words&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 1
##   word    
##   &amp;lt;chr&amp;gt;   
## 1 關關    
## 2 雎鳩    
## 3 在      
## 4 河      
## 5 之      
## 6 洲      
## 7 窈窕淑女
## 8 君子    
## 9 好逑
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;test %&amp;gt;%
  tibble(.name_repair = ~ &amp;quot;lines&amp;quot;) %&amp;gt;%
  unnest_tokens(word, lines, token = &amp;quot;characters&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 16 x 1
##    word 
##    &amp;lt;chr&amp;gt;
##  1 關   
##  2 關   
##  3 雎   
##  4 鳩   
##  5 在   
##  6 河   
##  7 之   
##  8 洲   
##  9 窈   
## 10 窕   
## 11 淑   
## 12 女   
## 13 君   
## 14 子   
## 15 好   
## 16 逑
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Changing the argument &lt;code&gt;token&lt;/code&gt; from &lt;code&gt;&amp;quot;words&amp;quot;&lt;/code&gt; to &lt;code&gt;&amp;quot;characters&amp;quot;&lt;/code&gt; shows that neither is the ideal output.
The first one does capture most of the disyllabic words (=good) but the problem really lies with the phrase 窈窕淑女, which is treated as one block in the first and as four pieces in the second.
Technically you can do more collocationwise with the second, but that&amp;rsquo;s not what I&amp;rsquo;m after here.&lt;/p&gt;

&lt;h2 id=&#34;quanteda&#34;&gt;&lt;code&gt;quanteda&lt;/code&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quanteda.corpus &amp;lt;- corpus(test)
tokens(quanteda.corpus)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## tokens from 2 documents.
## text1 :
## [1] &amp;quot;關關&amp;quot; &amp;quot;雎鳩&amp;quot; &amp;quot;、&amp;quot;   &amp;quot;在&amp;quot;   &amp;quot;河&amp;quot;   &amp;quot;之&amp;quot;   &amp;quot;洲&amp;quot;   &amp;quot;。&amp;quot;  
## 
## text2 :
## [1] &amp;quot;窈窕淑女&amp;quot; &amp;quot;、&amp;quot;       &amp;quot;君子&amp;quot;     &amp;quot;好逑&amp;quot;     &amp;quot;。&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gives the same problem: 窈窕淑女 is one block.
But maybe with a dictionary this problem can be solved?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;quant.dict &amp;lt;- dictionary(list(ideo = c(&amp;quot;關關&amp;quot;, &amp;quot;窈窕&amp;quot;)))
quanteda.toks &amp;lt;- tokens(quanteda.corpus)
tokens_lookup(quanteda.toks, dictionary = quant.dict, levels = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## tokens from 2 documents.
## text1 :
## [1] &amp;quot;ideo&amp;quot;
## 
## text2 :
## character(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dfm(quanteda.corpus, dictionary = quant.dict)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Document-feature matrix of: 2 documents, 1 feature (50.0% sparse).
## 2 x 1 sparse Matrix of class &amp;quot;dfm&amp;quot;
##        features
## docs    ideo
##   text1    1
##   text2    0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This isn&amp;rsquo;t really working &amp;ndash; the dictionary object in &lt;code&gt;quanteda&lt;/code&gt; is mostly something for further text analysis (after segmentation).
I do seem to remember there is a function that (&lt;a href=&#34;https://koheiw.net/?p=481&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;tokens_compound&lt;/code&gt;&lt;/a&gt;) that allows you to paste erroneously split words back together, but I don&amp;rsquo;t know if you can customize the cutting?&lt;/p&gt;

&lt;h2 id=&#34;jiebar&#34;&gt;&lt;code&gt;jiebaR&lt;/code&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cutter = worker()
segment(test, cutter)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;關關雎&amp;quot;   &amp;quot;鳩&amp;quot;       &amp;quot;在&amp;quot;       &amp;quot;河之洲&amp;quot;   &amp;quot;窈窕淑女&amp;quot; &amp;quot;君子好逑&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I wish I knew how to get the dictionary working, because then I would be able to just stay in R.
If anybody knows the fucntions, please tell me.
I would want something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jiebaR.dict &amp;lt;- c(&amp;quot;關關 5 id&amp;quot;, &amp;quot;窈窕 5 id&amp;quot;)
cutter2 &amp;lt;- worker(dict = jiebaR.dict)
segment(test, cutter2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;udpipe&#34;&gt;&lt;code&gt;udpipe&lt;/code&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#udmodel &amp;lt;- udpipe_download_model(language = &amp;quot;classical_chinese-kyoto&amp;quot;)
udmodel_KC &amp;lt;- udpipe_load_model(file = &amp;quot;classical_chinese-kyoto-ud-2.4-190531.udpipe&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- udpipe_annotate(udmodel_KC, x = test)
x &amp;lt;- as.data.frame(x)

tibble(x$token, x$upos)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 18 x 2
##    `x$token` `x$upos`
##    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;   
##  1 關        NOUN    
##  2 關雎      NOUN    
##  3 鳩        VERB    
##  4 、        PUNCT   
##  5 在        VERB    
##  6 河        NOUN    
##  7 之        SCONJ   
##  8 洲        VERB    
##  9 。        PUNCT   
## 10 窈        VERB    
## 11 窕        VERB    
## 12 淑        VERB    
## 13 女        PRON    
## 14 、        PUNCT   
## 15 君子      NOUN    
## 16 好        VERB    
## 17 逑        VERB    
## 18 。        PUNCT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow this is really weird.
It splits 關關雎鳩 as &lt;code&gt;關 關雎 鳩&lt;/code&gt; instead of &lt;code&gt;關關 雎鳩&lt;/code&gt;, and 窈窕淑女 as &lt;code&gt;窈 窕 淑 女&lt;/code&gt; instead of the desired &lt;code&gt;窈窕 淑女&lt;/code&gt;.
I don&amp;rsquo;t think I know how to improve this currently, as there are some dictionary settings that allow you to &lt;em&gt;suggest&lt;/em&gt; but not necessarily &lt;em&gt;enforce&lt;/em&gt; it.
Once again, if you know how, tell me now.&lt;/p&gt;

&lt;h2 id=&#34;jieba-in-python&#34;&gt;&lt;code&gt;jieba&lt;/code&gt; in python&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jieba
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seg_list = jieba.cut(&amp;quot;關關雎鳩、在河之洲。&amp;quot;, cut_all=False)
print(&amp;quot;Default Mode: &amp;quot; + &amp;quot;/ &amp;quot;.join(seg_list))  # 默认模式
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Default Mode: 關關/ 雎/ 鳩/ 、/ 在/ 河之洲/ 。
## 
## --- Logging error ---
## Traceback (most recent call last):
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py&amp;quot;, line 985, in emit
##     stream.write(msg)
## ValueError: I/O operation on closed file
## Call stack:
##   File &amp;quot;&amp;lt;string&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 301, in cut
##     for word in cut_block(blk):
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 233, in __cut_DAG
##     DAG = self.get_DAG(sentence)
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 179, in get_DAG
##     self.check_initialized()
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 168, in check_initialized
##     self.initialize()
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 111, in initialize
##     default_logger.debug(&amp;quot;Building prefix dict from %s ...&amp;quot; % (abs_path or &#39;the default dictionary&#39;))
## Message: &#39;Building prefix dict from the default dictionary ...&#39;
## Arguments: ()
## --- Logging error ---
## Traceback (most recent call last):
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py&amp;quot;, line 985, in emit
##     stream.write(msg)
## ValueError: I/O operation on closed file
## Call stack:
##   File &amp;quot;&amp;lt;string&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 301, in cut
##     for word in cut_block(blk):
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 233, in __cut_DAG
##     DAG = self.get_DAG(sentence)
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 179, in get_DAG
##     self.check_initialized()
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 168, in check_initialized
##     self.initialize()
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 145, in initialize
##     &amp;quot;Dumping model to file cache %s&amp;quot; % cache_file)
## Message: &#39;Dumping model to file cache /var/folders/kn/bjb0w7nx061145pnsxtwzpgc0000gn/T/jieba.cache&#39;
## Arguments: ()
## --- Logging error ---
## Traceback (most recent call last):
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py&amp;quot;, line 985, in emit
##     stream.write(msg)
## ValueError: I/O operation on closed file
## Call stack:
##   File &amp;quot;&amp;lt;string&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 301, in cut
##     for word in cut_block(blk):
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 233, in __cut_DAG
##     DAG = self.get_DAG(sentence)
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 179, in get_DAG
##     self.check_initialized()
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 168, in check_initialized
##     self.initialize()
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 163, in initialize
##     &amp;quot;Loading model cost %.3f seconds.&amp;quot; % (time.time() - t1))
## Message: &#39;Loading model cost 1.477 seconds.&#39;
## Arguments: ()
## --- Logging error ---
## Traceback (most recent call last):
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py&amp;quot;, line 985, in emit
##     stream.write(msg)
## ValueError: I/O operation on closed file
## Call stack:
##   File &amp;quot;&amp;lt;string&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 301, in cut
##     for word in cut_block(blk):
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 233, in __cut_DAG
##     DAG = self.get_DAG(sentence)
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 179, in get_DAG
##     self.check_initialized()
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 168, in check_initialized
##     self.initialize()
##   File &amp;quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/jieba/__init__.py&amp;quot;, line 164, in initialize
##     default_logger.debug(&amp;quot;Prefix dict has been built succesfully.&amp;quot;)
## Message: &#39;Prefix dict has been built succesfully.&#39;
## Arguments: ()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seg_list = jieba.cut(&amp;quot;窈窕淑女、君子好逑。&amp;quot;, cut_all=False)
#print(&amp;quot;Default Mode: &amp;quot; + &amp;quot;/ &amp;quot;.join(seg_list))  # 默认模式
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This doesn&amp;rsquo;t give the desired results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;a &amp;lt;-  c(&amp;quot;關關 5&amp;quot;, &amp;quot;窈窕 500&amp;quot;, &amp;quot;雎鳩&amp;quot;)
a
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;關關 5&amp;quot;   &amp;quot;窈窕 500&amp;quot; &amp;quot;雎鳩&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(r.a)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [&#39;關關 5&#39;, &#39;窈窕 500&#39;, &#39;雎鳩&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jieba.load_userdict(r.a)
#add_word(&#39;雎鳩&#39;, freq=None, tag=None)

seg_list = jieba.cut(&amp;quot;關關雎鳩、在河之洲。&amp;quot;, cut_all=False)
print(&amp;quot;Default Mode: &amp;quot; + &amp;quot;/ &amp;quot;.join(seg_list))  # 默认模式
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Default Mode: 關關/ 雎鳩/ 、/ 在/ 河之洲/ 。
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seg_list = jieba.cut(&amp;quot;窈窕淑女、君子好逑。&amp;quot;, cut_all=False)
print(&amp;quot;Default Mode: &amp;quot; + &amp;quot;/ &amp;quot;.join(seg_list))  # 默认模式
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Default Mode: 窈窕淑女/ 、/ 君子好逑/ 。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, it is really easy to just define a dictionary in R, because it is just a list (instead of feeding it a .txt file).
But I still don&amp;rsquo;t know how to enforce it.
Do I just change the weight, and if so to what setting?&lt;/p&gt;

&lt;h2 id=&#34;ckiptagger-in-python&#34;&gt;&lt;code&gt;ckiptagger&lt;/code&gt; (in python)&lt;/h2&gt;

&lt;p&gt;Last on the list is the recent &lt;code&gt;ckiptagger&lt;/code&gt; library.
According to the &lt;a href=&#34;https://github.com/ckiplab/ckiptagger&#34; target=&#34;_blank&#34;&gt;github page&lt;/a&gt; this model outperforms &lt;code&gt;jieba&lt;/code&gt;:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tool&lt;/th&gt;
&lt;th&gt;(WS) prec&lt;/th&gt;
&lt;th&gt;(WS) rec&lt;/th&gt;
&lt;th&gt;(WS) f1&lt;/th&gt;
&lt;th&gt;(POS) acc&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CkipTagger&lt;/td&gt;
&lt;td&gt;97.49%&lt;/td&gt;
&lt;td&gt;97.17%&lt;/td&gt;
&lt;td&gt;97.33%&lt;/td&gt;
&lt;td&gt;94.59%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CKIPWS (classic)&lt;/td&gt;
&lt;td&gt;95.85%&lt;/td&gt;
&lt;td&gt;95.96%&lt;/td&gt;
&lt;td&gt;95.91%&lt;/td&gt;
&lt;td&gt;90.62%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Jieba-zh_TW&lt;/td&gt;
&lt;td&gt;90.51%&lt;/td&gt;
&lt;td&gt;89.10%&lt;/td&gt;
&lt;td&gt;89.80%&lt;/td&gt;
&lt;td&gt;&amp;ndash;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I recommend following the steps for installation outlined over there because their tagged set is quite large (1.8 GB I or so), so you want that downloaded directly to your harddrive or a virtual environment or whatever it is the young kids do these days.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ckiptagger import *
#data_utils.download_data_gdown(&amp;quot;./&amp;quot;) # gdrive-ckip
#ws = WS(&amp;quot;./data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])
## /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or &#39;1type&#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / &#39;(1,)type&#39;.
##   np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ws = WS(&amp;quot;/Users/Thomas/data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;word_list = ws(
    r.test 
    )
    
word_list
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [[&#39;關關&#39;, &#39;雎鳩&#39;, &#39;、&#39;, &#39;在&#39;, &#39;河&#39;, &#39;之&#39;, &#39;洲&#39;, &#39;。&#39;], [&#39;窈窕淑女&#39;, &#39;、&#39;, &#39;君子&#39;, &#39;好逑&#39;, &#39;。&#39;]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is not ideal, but lets make a dictionary here too and run it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ideos &amp;lt;- c(&amp;quot;關關&amp;quot;, &amp;quot;窈窕&amp;quot;)
vals &amp;lt;- c(rep(5, times = 2))

lijst &amp;lt;- as.list(vals)
names(lijst) &amp;lt;- ideos

lijst
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $關關
## [1] 5
## 
## $窈窕
## [1] 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the format you want, because it matches the python format of &amp;lsquo;dictionaries&amp;rsquo; the best.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dictionario = construct_dictionary(r.lijst)
print(dictionario)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [(2, {&#39;關關&#39;: 5.0, &#39;窈窕&#39;: 5.0})]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;word_list = ws(
    r.test,
    coerce_dictionary = dictionario
    )
    
word_list
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [[&#39;關關&#39;, &#39;雎鳩&#39;, &#39;、&#39;, &#39;在&#39;, &#39;河&#39;, &#39;之&#39;, &#39;洲&#39;, &#39;。&#39;], [&#39;窈窕&#39;, &#39;淑女&#39;, &#39;、&#39;, &#39;君子&#39;, &#39;好逑&#39;, &#39;。&#39;]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By using the &lt;code&gt;coerce_dictionary&lt;/code&gt; argument, you &lt;em&gt;force&lt;/em&gt; this dictionary, to be used.
So theoretically it should look at that first before it throws other segmentation stuff at the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;py$word_list %&amp;gt;%
  #unlist()%&amp;gt;%
  enframe() %&amp;gt;%
  unnest(value) %&amp;gt;%
  group_by(name) %&amp;gt;%
  summarise(sentence = str_c(value, collapse = &amp;quot; &amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##    name sentence                   
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                      
## 1     1 關關 雎鳩 、 在 河 之 洲 。
## 2     2 窈窕 淑女 、 君子 好逑 。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Et voilà, segmented Classical Chinese, the way I want it.
(Well, I guess I would want 好逑 to be split in 好 and 逑 as well, but for now it&amp;rsquo;s okay.)&lt;/p&gt;

&lt;p&gt;The steps in this section thus consist of:
1. Providing target text (character vectors in R)
2. Making the dictionary (list in R)
3. Transforming the R dictionary to python dictionary (dictionary in python)
4. Running python script (import ckiptagger, load the ws data, run ws function with dictionary coerced)
5. Transform python object to nice dataframe in R (dataframe in R)&lt;/p&gt;

&lt;p&gt;I can readily see applications with a dictionary list taken from the &lt;a href=&#34;https://osf.io/kpwgf/&#34; target=&#34;_blank&#34;&gt;Chinese Ideophone Database CHIDEOD&lt;/a&gt;, and maybe with other databases connected to it as well.&lt;/p&gt;

&lt;p&gt;But I&amp;rsquo;m always open to hearing more ways of dealing with this preprocessing problem.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Above I&amp;rsquo;ve showcased a number of packages and ways to deal with the problem of Chinese Word Segmentation.
Here&amp;rsquo;s the score I would give them.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;package / library&lt;/th&gt;
&lt;th&gt;coding language&lt;/th&gt;
&lt;th&gt;score&lt;/th&gt;
&lt;th&gt;comment&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tidytext&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;if you want a quick solution (&amp;ldquo;characters&amp;rdquo;) or okayish solution (&amp;ldquo;words&amp;rdquo;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;quanteda&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;okayish solution, can&amp;rsquo;t split smaller?&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;jiebaR&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;how to use the dictionary function?&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;udpipe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;not developed enough / unclear instructions&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;jieba&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;python&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;with dictionaries you can get there, maybe; but how to enforce them?&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ckiptgagger&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;R + python&lt;/td&gt;
&lt;td&gt;9.5&lt;/td&gt;
&lt;td&gt;this method seems to get the ideophone job done, dictionaries can be enforced, but might also not be perfect?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;I hope you found this blog useful, but should you have tips on how to improve the workflow, always welcome.&lt;/strong&gt;
And thanks for sticking around until here.
As we say in Taiwan: 謝謝拜拜。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bridging phonology, meaning, and written form across time: introducing CHIDEOD, a database of Chinese literary ideophones</title>
      <link>/talk/2019-lund/</link>
      <pubDate>Sun, 05 May 2019 10:30:00 +0000</pubDate>
      <guid>/talk/2019-lund/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
