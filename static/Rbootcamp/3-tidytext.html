<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>R bootcamp for lexical semanticists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Thomas Van Hoey 司馬智" />
    <link rel="stylesheet" href="layout/ntu.css" type="text/css" />
    <link rel="stylesheet" href="layout/ntu-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# R bootcamp for lexical semanticists
## 3: <code>tidytext</code>
<html>
<div style="float:left">

</div>
<hr color='#875712' size=1px width=796px>
</html>
### Thomas Van Hoey 司馬智
### <br><br>7 November 2019<br>National Taiwan University

---




class: inverse, center, middle, clear

# Recap of last week

---

# Packages we learnt last week

.pull-left[
![](./figs/stickers/stringr.png)
]

.pull-right[
![](./figs/stickers/readr.png)
]

---

# Packages we will learn today

.pull-left[
![:scale 100%](./figs/stickers/tidytext.png)
]

.pull-right[
![](./figs/stickers/purrr.png)
]

---

# Packages we will learn today

.pull-left[
![](./figs/stickers/glue.png)
]

.pull-right[
![](./figs/stickers/lubridate.png)
]

---

# Things we will learn today

## Skills

* how to make a .blue[concordance] from a text tibble
* how to read in and process .blue[multiple files]
* how to make a .blue[wordcloud]

## Data
* how to download data from .blue[Project Gutenberg]
* how to work with .blue[multiple .Rds files] (as an example)

## Case studies
* the .blue[.sc[way construction]] (Goldberg 1995)
* the .blue[.sc[yidu 一度 construction]] (Chen 2019a; 2019b)  

These are mostly for simulating how to start working on .blue[getting data and cleaning it so we can start our linguistic analyses].

---

# Case study 1: The .sc[way construction]

Goldberg (1996: ch. 9) discusses the .sc[way construction].

Examples:

* *Frank found his way to New York.*
* *He belched his way out of the restaurant.*

She schematizes the construction as follows:

`$$[SUBJ_{i} [V [POSS_{i} way) OBL]]$$`

.purple[
Case study: You want to investigate her categorization and thus need to get your data ready for analysis

More specifically (for today), you want to investigate how [James Joyce](https://en.wikipedia.org/wiki/James_Joyce) 詹姆斯·乔伊斯 (1882-1941), the Irish novelist, used this construction in his book *Ulysses*.
]

---

# Step 1: data: Project Gutenberg

For old books (no longer under copyright), e.g. *Uylysses*,  we can use [Project Gutenberg](https://www.gutenberg.org) and the R package .orange[`gutenbergr`]. 

By now we know the drill:


```r
install.packages("gutenbergr")
```


```r
library(gutenbergr)
```

Searching for *Ulysses*, we get at this page: [click here](https://www.gutenberg.org/ebooks/4300).

There we can see that the number of *Ulysses* on Project Gutenberg is .blue[4300].
.grey[(You can see this in the link.)]

---

# Download *Ulysses*


```r
joyce &lt;- gutenbergr::gutenberg_download(gutenberg_id = 4300, 
                                        meta_fields = "title")
joyce 
```

After inspection, it seems they follow a printed book: not every line is a nice neat sentence.

---

# One sentence per line

So you maybe want to get one sentence per line.

Let us load in the relevant packages:


```r
library(tidytext)
library(tidyverse)
library(here)
```

With the .blue[`token = "sentences"`] argument setting you can turn the garbled lines into sentences (more or less, it is not 100% precise, but good enough for our purposes today)


```r
joyce_sent &lt;- joyce %&gt;%
  unnest_tokens(input = "text", 
                output = "sentence",
                token = "sentences") 
```

---

# Finding the .sc[way]

1st trial


```r
joyce_way &lt;- joyce_sent %&gt;%
  filter(str_detect(sentence, "way"))
joyce_way
```

--

2nd trial

Adding the word boundaries with .blue[`\\b`]


```r
## adding word boundaries with \\b
joyce_way2 &lt;- joyce_sent %&gt;%
  filter(str_detect(sentence, "\\bway\\b"))
```

Still not what we want.
We need to be as explicit as possible with our regular expression.

---

# Finding the .sc[way]

Let's see if we can learn something from Goldberg's schema:

`$$[SUBJ_{i} [V [POSS_{i} way) OBL]]$$`

So in the end we want:
* V verb
* POSS possessive
* OBL oblique

--

.blue[When you don't know how to continue, it helps to write a dummy example.]


```r
test &lt;- c("I lost my purse", "They lost their purse", "She lost her purse", "The other purse")

str_extract(test, "(my|their|her) purse")

# word boundaries with \\b
str_extract(test, "\\b(my|their|her)\\b purse")
```

---

# Finding the .sc[way]

3rd trial: implementing our dummy example, with some .orange[glue]:


```r
possessive &lt;- "\\bmy|your|his|her|our|their\\b"
way &lt;- "\\bway\\b"

library(glue)

joyce_wcx &lt;- joyce_way2 %&gt;%
  filter(str_detect(sentence, 
                    glue("\\b{possessive}\\b \\bway\\b"))
joyce_wcx
```

---

# Getting concordances

We know that we want `\\bway\\b` as our target word.
So we can make concordances with .red[mutate] and our .blue[regex] knowledge

Because what we want is something that looks like this:

left window | target word  | right window
----------- | -------------| -----------
Hello my | name | is Thomas .
Is that his | name |?
That's not my | name | .
His | name | is Jeff .


Option 1: 
We can either make a direct regular expression:


```r
# direct but difficult
regexdirect &lt;- "(\\w+\\s){0,9}\\b(my|your|his|her|our|their)\\b\\s\\bway\\b(\\s\\w+){0,10}"
```

---

# Getting concordances

Option 2: 
We can also *build* a regular expression (.blue[suggested])



```r
# building a good regular expression
possessive &lt;- "\\b(my|your|his|her|our|their)\\b"
way &lt;- "\\bway\\b"
left_window &lt;- "(\\w+\\s){0,9}"
right_window &lt;- "(\\s\\w+){0,10}"

regex &lt;- glue::glue("{left_window}{possessive}\\s{way}{right_window}")

# is it the same?
regex == regexdirect
```

---

# Getting concordances of *way*


```r
joyce_conc &lt;- joyce_wcx %&gt;%
  
  #str_extract_all because there might be more than one per line
  mutate(expression = str_extract_all(sentence, regex)) %&gt;%  
  
  # if you extract_all you should also unnest
  unnest(expression) %&gt;%
  
  # now we're making the left and right window
  mutate(left = str_replace_all(expression, glue("({possessive}).+"), ""),
         target = str_extract(expression, glue("{possessive}\\s{way}")),
         right = str_replace_all(expression, glue(".+({way})"), "")) %&gt;%
  
  # maybe better to split "his way" into "his" and "way"
  separate(target, into = c("poss", "way"), sep = " ") %&gt;%
  
  # only keep the relevant variables
  select(title, left, poss, way, right)
```

---

# Getting concordances of *way*

Let's check it out:


```r
joyce_conc
```

--

At first sight, it doesn't look like there's a lot of WAY construction going on in *Ulysses*.

Maybe we should .blue[widen the scope to other works by Joyce].

---

# More books by James Joyce

The .orange[gutenbergr] package can help out again.

First we need to find out all the .blue[id]s of the works by Joyce. As the [tutorial of .orange[gutenbergr]](https://ropensci.org/tutorials/gutenbergr_tutorial/) describes, we can use the .green[gutenberg_metadata] dataset that is loaded by the package to find that.


```r
gutenbergr::gutenberg_metadata %&gt;%
  filter(author == "Joyce, James") %&gt;%
  pull(gutenberg_id) -&gt; id

id
```

---

# Repeat steps from above

1. download the stuff from Project Gutenberg
2. .red[`tidytext::unnest_tokens()`] with .blue[`tokens = "sentences"`]


```r
joyce &lt;- gutenbergr::gutenberg_download(gutenberg_id = id, # we just found this out right!?
                                        meta_fields = "title")

# it may possibly not be able to download all the books, but that's okay for today

joyce %&gt;% count(title)

joyce_sent &lt;- joyce %&gt;%
  unnest_tokens(input = "text", 
                output = "sentence",
                token = "sentences")  
```

---

# Repeat steps from above

1. download the stuff from Project Gutenberg
2. .red[`tidytext::unnest_tokens()`] with .blue[`tokens = "sentences"`]
3. run the regular expression to .red[filter]
4. work the .blue[concordance] functions


```r
regex # check if regex is still there

joyce_regex &lt;- joyce_sent %&gt;%
  filter(str_detect(sentence, regex))

joyce_conc &lt;- joyce_regex %&gt;%
  mutate(expression = str_extract_all(sentence, regex)) %&gt;%  
  unnest(expression) %&gt;%
  mutate(left = str_replace_all(expression, glue("({possessive}).+"), ""),
         target = str_extract(expression, glue("{possessive}\\s{way}")),
         right = str_replace_all(expression, glue(".+({way})"), "")) %&gt;%
  separate(target, into = c("poss", "way"), sep = " ") %&gt;%
  select(title, left, poss, way, right)
joyce_conc
```

---

# Export the file

Now we have a beautiful (though not superbig) dataset to further code in other software.
I would choose a csv file.


```r
write_csv(joyce_conc, 
          here::here("output", "joyce_way.csv"))
```




---

# Analysis

So this is where you .blue[interact with your data].

As I said in the first session: you already are a data scientist because you have interacted with your data before.
What we do here is finding ways to .blue[do the dirty work] in a sense: finding lots of data, cleaning it and making it ready for analysis.

For the case study of the .sc[way construction]: you may want to check every line to make sure it is part of the .sc[way construction] that Goldberg intended, cf.

*Frank found his way to New York.*

--

I did this for you (with .blue[yes/no] as the values)

(Hopefully you downloaded the zip files I asked you to download before class).

Save the file "joyce_way_tagged.csv" in .blue[Rbootcamp &gt; inputfiles]

---

# Analysis


```r
joyce_cx &lt;- read_csv("inputfiles", "joyce_way_tagged.csv") #cx for construction
```




`$$[SUBJ_{i} [V [POSS_{i} way) OBL]]$$`

So we are probably interested in 
* POSS: the possessive, *his*
* V: the verb and 
  * Verb tokens (as they appear in our data), e.g. *saw*
  * Verb types (by .blue[stemming] or .blue[lemmatization]), e.g. *see*
* OBL: the oblique arguments, e.g. *through*

Some possible difference between stemming and lemmatizing:

word | stemming | lemmatizing
---- | -------- | -------
saw  | saw      | see
broken | broke | break 


---

# Analysis


```r
# for stemming: 
library(SnowballC) 

joyce_cx %&gt;%
  filter(construction == "yes") %&gt;%
  mutate(oblique = str_replace_all(right, "^(\\w+\\b).+", "\\1")) %&gt;%
  mutate(verbtoken = str_replace_all(left, ".+\\s(\\w+)$", "\\1")) %&gt;%
  mutate(verbtype = SnowballC::wordStem(verbtoken,
                                        language = "english")) -&gt; joyce_df
```

If you want to not just stem the words, but actually lemmatize them, [this blog](http://www.bernhardlearns.com/2017/04/cleaning-words-with-r-stemming.html) recommends you the .orange[koRpus] package.
Check it out if you want to change e.g. *saw* to *see* (what we would want).

What you could also do is use an existing corpus, e.g. *British National Corpus BNC* or *Corpus Of Contemporary American English COCA* and basically get all the verbs and then extract their base form as well as the token forms into a very long list and then .red[dplyr::distinct()] that list, and then join that to the .blue[verbtoken] variable we made before.

---

# Analysis

Let's inspect it:


```r
joyce_df
```

Now we can get a number of statistics, for example


```r
joyce_df %&gt;% count(verbtype, sort = TRUE)
joyce_df %&gt;% count(poss, sort = TRUE)

joyce_df %&gt;% count(verbtype, oblique)
joyce_df %&gt;% count(verbtype, poss)

# the sky is the limit, or rather the data is the limit
```

For something like the .sc[way construction], you could e.g. connect it to .sc[force dynamics] or to metaphorical usage, **or to ...**

My hope for you is that the hardest part of analysis is not getting the data and counting things, but rather the .blue[coding and interpretation, when you ask questions to your data].

---

# Case study 1: wordclouds!

We'll use a new package


```r
library(ggwordcloud)
```

If we are just interested in a nice fun visualization, we can use wordclouds.

Let us .red[`unnest_tokens`] with the .blue[`token = "words"`] argument:


```r
# things we probably still have active in memory
#joyce &lt;- gutenbergr::gutenberg_download(gutenberg_id = id, meta_fields = "title")

joyce %&gt;% count(title)

joyce_words &lt;- joyce %&gt;%
  unnest_tokens(input = "text", 
                output = "words",
                token = "words")  
joyce_words
```

---

# Wordclouds!

.orange[tidytext] comes with a .green[stop_words] dataset (for English).

We can use .orange[dplyr::].red[anti_join] to take these stop words out of our dataset.

Below .blue[`words`] is in `joyce_words` and .blue[`word`] is in `stop_words`


```r
joyce_anti &lt;- joyce_words %&gt;%
  anti_join(stop_words, by = c("words" = "word"))
```



```r
joyce_anti %&gt;%
  count(title, words) %&gt;%
  filter(title == "Dubliners") %&gt;%
  arrange(desc(n)) %&gt;%
  top_n(50) %&gt;%
  ggplot(aes(label = words, size = n)) + 
  geom_text_wordcloud() +
  theme_minimal()
```

---

# Wordclouds per book! 

Let's do this per book.

For this we need to .orange[dplyr::].red[group_by()] the title variable (instead of filtering) and later we will use .orange[ggplot2::].red[facet_wrap()] to plot the results in as many windows as there are titles (about 4).


```r
joyce_anti %&gt;%
  count(title, words) %&gt;%
  group_by(title) %&gt;%
  arrange(desc(n)) %&gt;%
  top_n(50) %&gt;%
  ggplot(aes(label = words, size = n, color = title)) + 
  geom_text_wordcloud() +
  facet_wrap(facets = "title") +
  theme_minimal()
```

---

# Homework

Use part of the script below to read in a word document (or group of word documents with what we learn in the second case study) and try to use .red[`unnest_tokens`] to get all the words, and then try to visualize it in a a wordcloud with .orange[ggwordcloud] and .orange[ggplot2].

ALL THE CAPITALS need to be replaced by you!



```r
library(textreadr)

p &lt;- PATH_TO_YOUR_DOCUMENT #don't forget the  "" !

doc &lt;- textreadr::read_docx(p)

doc %&gt;%
  enframe() %&gt;% # turn vector of strings into nice tidy table
  unnest_tokens(input = "value", 
                output = "words",
                token = "words") %&gt;%
  COUNT %&gt;%
  GGPLOT +
  GEOM_TEXT_WORDCLOUD +
  theme_minimal()
```

---

# Case study 2: 一度

To help Abner, and also to demonstrate the techniques we learned, we are going to get all sentences with 一度 from the [Academia Sinica Balanced Corpus ASBC 4.0](http://asbc.iis.sinica.edu.tw)

Let us look at some statistics on that website, and what we get if we just search for 「一度」.


class (genre) | tokens
--------------| ------
文學	| 2
生活	| 54
社會	| 242
科學	| 6 
哲學	| 9
藝術  | 5

So in total there are 318 tokens.

.purple[Can we do better with R and the data itself?]


---

class: inverse, center, middle, clear

# Break + &lt;br&gt; making sure you have the &lt;br&gt; tidy_ASBC_unnested files

---

# From one to many

.font140[

Important lesson:

.red[
In programming languages, you almost always first want to test if your code works for 1 file, and only then run that code on all files.
]

But

Also remember to be DRY:

.red[Don't Repeat Yourself]
]

If you have to reuse the same code three times, it is time to write a function so you can only make one mistake (and write solutions) once.


---

# From one to many

Let's say we want to test the calculator in R.


```r
some_numbers &lt;- c(2, 5, 29, 43)
some_numbers + 1
```

```
## [1]  3  6 30 44
```

```r
some_numbers + 1 + 1
```

```
## [1]  4  7 31 45
```

```r
some_numbers + 1 + 1 + 1
```

```
## [1]  5  8 32 46
```

Oh no, we have written this three times! What if in the future we want not to just .red[`+ 1`] but want to .red[`+ 2`]?

We will need to write a function that we can change.

---

# From one to many: functions

This is how you write a function:


```r
plus_one &lt;- function(x){
  x + 1
}
```

Look a function has appeared in our Environment tab!

Let's try it:


```r
plus_one(some_numbers)
```

--

Changing it:


```r
plus_one &lt;- function(x){
  x + 2
}

plus_one(some_numbers)
```

---

# Reading in the .Rds files

We need to first read in our files and try to find our search expression.

We can read in a .blue[directory (a folder)] of files by using .orange[fs::].red[dir_ls]


```r
files &lt;- fs::dir_ls(here("ASBC"), # or check path
                    regexp = ".rds$",
                    recurse = TRUE)
files
```

I know that in filenumber 203976.rds there should be an 一度, but otherwise you should guess a bit, or first try with a high frequency word like 我:


```r
files %&gt;% str_which("203976")

# the 846th file, this is a Base R expression
files[846]

testrds &lt;- read_rds(files[846])
```

---

# Filtering for one 一度


```r
testrds %&gt;%
  filter(str_detect(text, "一度"))
```

---

# Filtering for all 一度


```r
# takes about a minute for me
files &lt;- fs::dir_ls(here("ASBC"),
                    regexp = ".rds$",
                    recurse = TRUE)

# first just find all the 一s, maybe we want more
expr &lt;- "\\b一"

# these are the steps we need to take per file:
finder &lt;- function(file){
  file %&gt;%
    read_rds() %&gt;%
    filter(str_detect(text, expr))
}
```


---

# Filtering for all 一度

It's time we meet .orange[purrr], the R cat that iterates over many things.

This R cat wants:
* things to loop / iterate over 
* a function to do in the loop
* you get to choose the output:
  * .red[map] gives a list
  * .red[map_df] gives a dataframe (tibble)
  * there is a [cheat sheet](https://rstudio.com/resources/cheatsheets/) 'Apply Functions Cheat Sheet' &lt;br&gt; (but you don't need to download it now)

So to get the dataframe of all the sentences containing 一:


```r
df &lt;- map_df(files, finder)
```

---

# Narrowing the field

Given that this is a tagged corpus, all words are stored as .blue[WORD_TAG]

So logically there could be two options for these two characters:
* 一_TAG 度_TAG
* 一度_TAG

Let's make a regular expression:


```r
# I did a few runs for this, not in one go :) 
regex &lt;- "一_\\w+\\b\\s度_\\w+\\b|一度_\\w+\\b"

yidu &lt;- df %&gt;%
  filter(str_detect(text, regex))

yidu
```

---

# Concordances for 一度

Version .blue[with parts-of-speech tags POS] and without NOPOS


```r
# copy from James Joyce above and adapt!

yidu_pos &lt;- yidu %&gt;%
  #mutate(expression = str_extract_all(text, glue::glue(".+{regex}.+"))) %&gt;% 
  #unnest(expression) %&gt;%
  mutate(left = str_replace_all(text, glue::glue("({regex}).+"), ""),
         target = str_extract(text, glue::glue("{regex}")),
         right = str_replace_all(text, glue::glue(".+({regex})"), "")) 

yidu_pos %&gt;%
  select(left, target, right, text)
```

---

# Concordances for 一度

Version with parts-of-speech tags POS and .blue[without NOPOS]



```r
regex2 &lt;- "一\\s?度"

yidu_nopos &lt;- yidu %&gt;%
  mutate(text = str_replace_all(text, "_\\w+\\b", "")) %&gt;%
  #mutate(expression = str_extract_all(text, regex2)) %&gt;%  
  #unnest(expression) %&gt;%
  mutate(left = str_replace_all(text, glue("({regex2}).+"), ""),
         target = str_extract(text, glue("{regex2}")),
         right = str_replace_all(text, glue(".+({regex2})"), ""))

yidu_nopos
```

---

# Is R better than online corpus?

class (genre) | tokens
--------------| ------
文學	| 2
生活	| 54
社會	| 242
科學	| 6 
哲學	| 9
藝術  | 5

versus: 


```r
yidu %&gt;%
  count(class)
```

---

# More and more 一度s?

.purple[We want to know if the number of 一度s increased over the years.]

Normally you want not just absolute (token) frequencies, but relative frequencies, but today we are just going to look at token frequency.


```r
yidu_nopos %&gt;%
  count(publishdate)
```

Oh oh, the publishdate is very messy. 
You could try to programmatically (with R) change it into the same structure, but since it's only 186 rows at this point, I would consider doing it manually.

Although if year is enough, we might be able to do it quick and dirty!


```r
yidu_nopos %&gt;%
  count(publishdate) %&gt;%
  arrange(desc(year))
```

---

# More and more 一度s?

Western years


```r
yidu_nopos %&gt;%
  count(publishdate) %&gt;%
  mutate(year = case_when(
    # western dates
    str_detect(publishdate, "198.") ~ str_extract(publishdate, "198."),
    str_detect(publishdate, "199.") ~ str_extract(publishdate, "199."),
    str_detect(publishdate, "200.") ~ str_extract(publishdate, "200."),
    TRUE ~ "test"
  )) %&gt;%
  arrange(desc(year))
```

---

# More and more 一度s?

Western years and Taiwanese years


```r
yidu_nopos %&gt;%
  count(publishdate) %&gt;%
  mutate(year = case_when(
    # western dates
    str_detect(publishdate, "198.") ~ str_extract(publishdate, "198."),
    str_detect(publishdate, "199.") ~ str_extract(publishdate, "199."),
    str_detect(publishdate, "200.") ~ str_extract(publishdate, "200."),
    # taiwanese dates
    str_detect(publishdate, "84..") ~ "1995",
    str_detect(publishdate, "85..") ~ "1996",
    str_detect(publishdate, "86..") ~ "1997",
    str_detect(publishdate, "87..") ~ "1998",
    str_detect(publishdate, "88..") ~ "1999",

    TRUE ~ "test"
  )) %&gt;%
  arrange(desc(year))
```

---

# More and more 一度s?

Final form (with the .blue[`\\d`] regular expression)


```r
yidu_nopos %&gt;%
  mutate(year = case_when(
    # western dates
    str_detect(publishdate, "198\\d") ~ str_extract(publishdate, "198."),
    str_detect(publishdate, "199\\d") ~ str_extract(publishdate, "199."),
    str_detect(publishdate, "200\\d") ~ str_extract(publishdate, "200."),
    # taiwanese dates
    str_detect(publishdate, "84\\d\\d") ~ "1995",
    str_detect(publishdate, "85\\d\\d") ~ "1996",
    str_detect(publishdate, "86\\d\\d") ~ "1997",
    str_detect(publishdate, "87\\d\\d") ~ "1998",
    str_detect(publishdate, "88\\d\\d") ~ "1999",

    TRUE ~ "test"
  )) %&gt;%
  count(year) %&gt;%
  filter(year != "test") -&gt; predate
```

---

# Lubridate

Now we could turn the .blue[year] variable into a date (a special data object class in R). You can use .orange[lubridate::].red[ymd()] for that. 

But the code below will add "01-01" to our dates (but for current purposes that is okay)


```r
library(lubridate)

predate %&gt;% 
  mutate(year = as.numeric(year),
         year = lubridate::ymd(year, truncated = 2L)) -&gt; dated_yidu
```

Let's visualize!


```r
dated_yidu %&gt;%
  ggplot(aes(x = year, y= n)) +
  geom_point() +
  theme_minimal() +
  geom_smooth(stat = "smooth")
```


---

# Final showcase

A short demonstration on how to use .red[`unnest_tokens()`] with .blue[segmented Chinese text].

Option 1: .blue[`token = "words"`] argument


```r
yidu_nopos %&gt;%
  select(class, text) %&gt;%
  unnest_tokens(input = "text",
                output = "words",
                token = "words") %&gt;%
  count(words, sort = TRUE)
```

---

# Final showcase

A short demonstration on how to use .red[`unnest_tokens()`] with .blue[segmented Chinese text].


Option 2:
It is generally safer to use the .blue[`token = "regex"`] and .blue[`pattern = "\\s"`] arguments in .red[`unnest_tokens()`].


```r
yidu_nopos %&gt;%
  select(class, text) %&gt;%
  unnest_tokens(input = "text",
                output = "words",
                token = "regex",
                pattern = "\\s") %&gt;%
  count(words, sort = TRUE)
```


---

# What did we learn today?

## Skills

* ✔️ how to make a .blue[concordance] from a text tibble
* ✔️ how to read in and process .blue[multiple files]
* ✔️ how to make a .blue[wordcloud]

## Data
* ✔️ how to download data from .blue[Project Gutenberg]
* ✔️ how to work with .blue[multiple .Rds files] (as an example)

## Case studies
* ✔️ the .blue[.sc[way construction]] (Goldberg 1995)
* ✔️ the .blue[.sc[yidu 一度 construction]] (Chen 2019a; 2019b)  


---

# Next week

## Skills

* .orange[quanteda] package .grey[(easier corpus but we needed .orange[tidytext] first)]
* scraping stuff from static websites with .orange[rvest]
* looking at some .blue[.xml files]

## Datasets

* a .blue[website of your choice] (you can tell me on CEIBA)
* .blue[PTT]
* (a part of the) .blue[BNC corpus (in xml)]

## Case studies

* .blue[Association measures from Gries]: your homework is to read the paper(s).

---


background-image: url(https://media.giphy.com/media/upg0i1m4DLe5q/source.gif)
background-position: 50% 50%
background-size: 100%
class: center, bottom, clear
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="./layout/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
