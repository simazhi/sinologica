---
title: Guanguan goes the Chinese Word Segmentation (II)
author: Thomas Van Hoey
date: '2019-09-11'
slug: guanguan-goes-the-chinese-word-segmentation-2
categories:
  - Blog
tags:
  - R
  - python
  - coding
  - classics
  - CHIDEOD
subtitle: ''
summary: ''
authors: []
lastmod: '2019-09-11T12:27:22+08:00'
featured: no
header:
  image:
  caption: ''
  focal_point: ''
  preview: yes
projects: []
---

# tl; dr

This double blog is first about the opening line of the *Book of Odes*, and later about how to deal with Chinese word segmentation, and my current implementation of it. So if you're only [interested in the computational part, look at the next one](../guanguan-goes-the-chinese-word-segmentation-2). If, on the other hand, you want to know more about my views on the translation of *guān guān* etc., [look at the first part](../guanguan-goes-the-chinese-word-segmentation).
In this part I use different approaches from a mostly R-centred focus to look at word segmentation in Chinese.

# Intro - quick recap

The issues that prompted me to write the previous post are twofold.
On the one hand, I came across a translation of the first line of the *Book of Odes* (*Shījīng* 詩經) and subsequent critiques of that translation. 
I decided to give my take on the extensive coverage of the first stanza.
Here is the first line as it is transmitted to us, as well with a translation I made.
(By the way, if you can provide a better translation, please bring it on.)


>關關雎鳩，  
在河之洲。  
窈窕淑女，  
君子好逑。   

> *Krōn, krōn* the físh-hawks cáll,  
ón the íslet ín the ríver.  
délicáte, demúre, young lády,  
fór the lórd a góód mate shé.

On the other hand, I have been reading up on how to deal with Chinese from a corpus linguistics point-of-view.
And that means also looking at how Natural Language Processing -- the next-door-neighbour of corpus linguistics -- deals with these issues.
To see why they are next-door-neighbours and not the same, you can read Gries's (2011) "Methodological and interdisciplinary stance in Corpus Linguistics".

Anyway, there are *a lot* of things you can do with corpora, also in Chinese, but in general there are a few steps you need to do before you can even begin analyzing linguistic data and throwing different models at the data.

# Steps involved in text analysis

[Welbers et al. (2017)](10.1080/19312458.2017.1387238) discuss the steps generally involved in text analysis, with a particular focus on [R](https://www.r-project.org).
These can be subsumed in three groups of tasks, and are exemplified with some R packages that may do the trick for that particular task.

![](/img/2019/Welbers2017.png)

**But**, what they forgot is that not every language comes with nicely defined space between the units (not necessarily words, because that concept is also a bit fuzzy). 
Chinese and Japanese are prime examples of this phenomenon.
A really quick google search led me to this [Quora post where they asked what the differences are between Chinese and English NLP (Natural Language Processing)](https://www.quora.com/What-are-the-main-differences-between-NLP-for-Chinese-vs-NLP-for-English), and the answers, provided a certain Chier Hu are pretty good ([check it out yo](https://qr.ae/TWKny4)). 

What I'm trying to get at here is that you need to break up long strings of Chinese first before you can even about putting things in the language modelling mixer.
So **below I am going to discuss a bit how I went about SEGMENTATION in the past, what some alternatives are, and what I'm doing now.**
**If you have any suggestions to improve this workflow, please contact me!**

# The alternatives

## Monosyllabic treatment

Since the text I'm dealing with is Classical Chinese / Old Chinese, my requirements are even a bit stricter than those for Modern Chinese.
While you can argue that, since Classical Chinese is largely monosyllabic (one character = one word), this is actually not entirely true.
Some examples include ideophones as *guānguān* 關關 and *yáotiáo* 窈窕, but also words like *jūnzi* 君子, all present in this stanza.
So what I want is a segmentation tool that ideally can handle this, or handle this with some help from me.

I stumbled across [this tidy approach to Classical Chinese](http://jjohn987.rbind.io/post/a-quasi-tidytext-analysis-of-3-chinese-classics/) that exactly falls into this trap, and why there may be sometimes good reasons to just place a space after every character, I can't do that because I look at often polysyllable words like the ones exemplified above.
So I'll give this an A for exploratory analysis, but a B for segmentation.

## Keeping with the tidy framework

Without doubt, Julia Silge's [tidytext](https://www.tidytextmining.com/tidytext.html) approach is a big improvement in the way of thinking about text mining principles.
Her main function `unnest_tokens` allows for multiple units, and the previous approach uses `token = "characters"` as the main unit.
I think for the steps after segmentation, further along the analysis pipeline, this package really shines, and you can include other parameters like `token = "words"` or `token = "skipgram"` as well. 
The latter is actually a function I am using in my dissertation.
But the problem lies with the segmentation, which is thus not really solved by this package.

## Older, well-known packages






https://quanteda.io/articles/pkgdown/comparison.html

# Setting up the workspace and test materials

## Loading in the required libraries
```{r packages}
library(tidyverse) #general catch-all of the tidyverse
```

## Test material

As test material I just care about this stanza from the *Shījīng*.

```{r test}
test <- c("關關雎鳩、在河之洲。",
          "窈窕淑女、君子好逑。")
```



```{r}
gov_reports <- "https://api.github.com/repos/ropensci/textworkshop17/contents/demos/chineseDemo/govReports"
raw <- httr::GET(gov_reports)
paths <- sapply(httr::content(raw), function(x) x$path)
names <- tools::file_path_sans_ext(basename(paths))
urls <- sapply(httr::content(raw), function(x) x$download_url)
text <- sapply(urls, function(url) paste(readLines(url, warn = FALSE,
                                                   encoding = "UTF-8"),
                                         collapse = "\n"))
names(text) <- names

toks <- stringi::stri_split_boundaries(text, type = "word")
dict <- unique(c(toks, recursive = TRUE)) # unique words
text2 <- sapply(toks, paste, collapse = "\u200b")
text2
```


